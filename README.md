# VC4VG: Optimizing Video Captions for Text-to-Video Generation

<div style='display:flex; gap: 0.25rem; '>
  <a href='https://arxiv.org/pdf/your_paper_id.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a>
  <a href='https://github.com/qyr0403/VC4VG'><img src='https://img.shields.io/badge/Project-Page-green'></a>
  <a href='https://huggingface.co/your_hf_id'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-VC4VG-blue'></a>
</div>

<p align="center" width="100%">
<a target="_blank"><img src="https://raw.githubusercontent.com/qyr0403/VC4VG/main/assets/framework.png" alt="VC4VG Framework" style="width: 80%; min-width: 200px; display: block; margin: auto;"></a>
</p>

> **VC4VG: Optimizing Video Captions for Text-to-Video Generation** <br>
> Yang Du*, Zhuoran Lin*, Kaiqiang Song*, Biao Wang, Zhicheng Zheng, Tiezheng Ge, Bo Zheng, Qin Jin <br>
[![github](https://img.shields.io/badge/-Github-black?logo=github)](https://github.com/qyr0403/VC4VG) [![github](https://img.shields.io/github/stars/qyr0403/VC4VG.svg?style=social)](https://github.com/qyr0403/VC4VG)[![arXiv](https://img.shields.io/badge/Arxiv-your_paper_id-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/your_paper_id) <br>

## Overview

Text-to-video (T2V) generation models rely on high-quality video-text training sets for enhancing instruction-following capabilities and improving the overall quality of generated videos. Existing video captioning work lacks a systematic optimization framework designed specifically from the perspective of T2V generation needs. 

### Contributions of VC4VG

*   **Decompose video captioning into five key dimensions crucial for video reconstruction.** We break down video captioning into Subject Attributes, Subject Actions, Environment Attributes, Environment Changes, Camera, and Stylization.
*   **Propose [VC4VG-Bench](https://github.com/qyr0403/VC4VG), a new automatic benchmark with 1,000 QA pairs.** This benchmark is designed to evaluate captions based on their suitability for T2V generation.
*   **Validate our optimization framework through a proof-of-concept and T2V fine-tuning experiments.** We show that fine-tuning a T2V model with captions generated by our framework leads to higher-quality video generation.

## Key Findings

Our experiments demonstrate that:

*   **VC4VG can guide model optimization to generate higher quality video captions.** Our proof-of-concept model developed through VC4VG, shows the effectiveness of our training strategy.
*   **Training a T2V model with VC4VG's higher-quality captions directly leads to higher-quality video generation.** This is validated by both automated metrics and human evaluation.
*   **VC4VG-Bench effectively evaluates captions with generation-oriented metrics.** It achieves over 80% consistency with human judgment through a dual-reference human annotation strategy.

## Citation

If you find our work useful, please consider citing our paper:

```bibtex
@misc{du2025vc4vg,
      title={VC4VG: Optimizing Video Captions for Text-to-Video Generation}, 
      author={Yang Du and Zhuoran Lin and Kaiqiang Song and Biao Wang and Zhicheng Zheng and Tiezheng Ge and Bo Zheng and Qin Jin},
      year={2025},
      eprint={your_paper_id},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
